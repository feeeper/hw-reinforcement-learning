{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Project. OpenAI Taxi-v3 env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Описание задачи"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Доставить пассажира с места вызова такси до пункта назначения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Состояния\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возможны 500 состояний: 25 стартовых позиций (поле 5х5 клеток), 5 позиций пассажира, 4 возможных пункта назначения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Действия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В окружении определены 6 детерминированных действия:\n",
    "- `0` &mdash; ехать на юг\n",
    "- `1` &mdash; ехать на север\n",
    "- `2` &mdash; ехать на восток\n",
    "- `3` &mdash; ехать на запад\n",
    "- `4` &mdash; подобрать пассажира\n",
    "- `5` &mdash; высадить пассажира"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Награда"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Возможная награда:\n",
    "- `-1` за каждый шаг в стратегии, если не положена иная награда\n",
    "- `+20`, если пассажир доставлен\n",
    "- `-10`, если действия \"подобрать пассажира\" (`4`) или \"высадить пассажира\" (`5`) выполнены в \"неправильных\" состояниях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "import random\n",
    "\n",
    "tqdm.monitor_interval = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[35mR\u001b[0m: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration and Value Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_evaluation(policy, environment, discount_factor=1.0, theta=1e-9, max_iterations=1e9):\n",
    "    # Number of evaluation iterations\n",
    "    evaluation_iterations = 1\n",
    "    # Initialize a value function for each state as zero\n",
    "    V = np.zeros(environment.nS)\n",
    "    # Repeat until change in value is below the threshold\n",
    "    for i in range(int(max_iterations)):\n",
    "        # Initialize a change of value function as zero\n",
    "        delta = 0\n",
    "        # Iterate though each state\n",
    "        for state in range(environment.nS):\n",
    "           # Initial a new value of current state\n",
    "           v = 0\n",
    "           # Try all possible actions which can be taken from this state\n",
    "           for action, action_probability in enumerate(policy[state]):\n",
    "             # Check how good next state will be\n",
    "             for state_probability, next_state, reward, terminated in environment.P[state][action]:\n",
    "                  # Calculate the expected value\n",
    "                  v += action_probability * state_probability * (reward + discount_factor * V[next_state])\n",
    "\n",
    "           # Calculate the absolute change of value function\n",
    "           delta = max(delta, np.abs(V[state] - v))\n",
    "           # Update value function\n",
    "           V[state] = v\n",
    "        evaluation_iterations += 1\n",
    "\n",
    "        # Terminate if value change is insignificant\n",
    "        if delta < theta:\n",
    "            print(f'Policy evaluated in {evaluation_iterations} iterations.')\n",
    "            return V\n",
    "\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_step_lookahead(environment, state, V, discount_factor):\n",
    "    action_values = np.zeros(environment.nA)\n",
    "    for action in range(environment.nA):\n",
    "        for probability, next_state, reward, terminated in environment.P[state][action]:\n",
    "            try:\n",
    "                action_values[action] += probability * (reward + discount_factor * V[next_state])\n",
    "            except TypeError as te:\n",
    "                print(action_values, V)\n",
    "                raise te\n",
    "    return action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(environment, discount_factor=1.0, max_iterations=1e9):\n",
    "    # Start with a random policy\n",
    "    #num states x num actions / num actions\n",
    "    policy = np.ones([environment.nS, environment.nA]) / environment.nA\n",
    "    # Initialize counter of evaluated policies\n",
    "    evaluated_policies = 1\n",
    "    # Repeat until convergence or critical number of iterations reached\n",
    "    for i in tqdm.tqdm(range(int(max_iterations)), 'policy iteration'):\n",
    "        stable_policy = True\n",
    "        # Evaluate current policy\n",
    "        V = policy_evaluation(policy, environment, discount_factor=discount_factor, max_iterations=100)\n",
    "        # Go through each state and try to improve actions that were taken (policy Improvement)\n",
    "        for state in range(environment.nS):\n",
    "            # Choose the best action in a current state under current policy\n",
    "            current_action = np.argmax(policy[state])\n",
    "            # Look one step ahead and evaluate if current action is optimal\n",
    "            # We will try every possible action in a current state\n",
    "            action_value = one_step_lookahead(environment, state, V, discount_factor)\n",
    "            # Select a better action\n",
    "            best_action = np.argmax(action_value)\n",
    "            # If action didn't change\n",
    "            if current_action != best_action:\n",
    "                stable_policy = False\n",
    "                # Greedy policy update\n",
    "                policy[state] = np.eye(environment.nA)[best_action]\n",
    "        evaluated_policies += 1\n",
    "        # If the algorithm converged and policy is not changing anymore, then return final policy and value function\n",
    "        if stable_policy:\n",
    "            print(f'Evaluated {evaluated_policies} policies.')\n",
    "            return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(environment, discount_factor=1.0, theta=1e-9, max_iterations=1e9):\n",
    "    # Initialize state-value function with zeros for each environment state\n",
    "    V = np.zeros(environment.nS)\n",
    "    for i in tqdm.tqdm(range(int(max_iterations)), 'value iteration'):\n",
    "        # Early stopping condition\n",
    "        delta = 0\n",
    "        # Update each state\n",
    "        for state in range(environment.nS):\n",
    "            # Do a one-step lookahead to calculate state-action values\n",
    "            action_value = one_step_lookahead(environment, state, V, discount_factor)\n",
    "            # Select best action to perform based on the highest state-action value\n",
    "            best_action_value = np.max(action_value)\n",
    "            # Calculate change in value\n",
    "            delta = max(delta, np.abs(V[state] - best_action_value))\n",
    "            # Update the value function for current state\n",
    "            V[state] = best_action_value\n",
    "            # Check if we can stop\n",
    "        if delta < theta:\n",
    "            print(f'Value-iteration converged at iteration#{i}.')\n",
    "            break\n",
    "\n",
    "    # Create a deterministic policy using the optimal value function\n",
    "    policy = np.zeros([environment.nS, environment.nA])\n",
    "    for state in range(environment.nS):\n",
    "        # One step lookahead to find the best action for this state\n",
    "        action_value = one_step_lookahead(environment, state, V, discount_factor)\n",
    "        # Select best action based on the highest state-action value\n",
    "        best_action = np.argmax(action_value)\n",
    "        # Update the policy to perform a better action at a current state\n",
    "        policy[state, best_action] = 1.0\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte-Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_state_action_dictionary(env):\n",
    "    Q = {}\n",
    "    for key in range(env.observation_space.n):\n",
    "        Q[key] = {a: 0.0 for a in range(0, env.action_space.n)}\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_game(env, policy, display=True):\n",
    "    state = env.reset()\n",
    "    frames = []\n",
    "    finished = False\n",
    "    \n",
    "    while not finished:\n",
    "        s = env.env.s\n",
    "        if display:\n",
    "            clear_output(True)\n",
    "            env.render()\n",
    "            sleep(1)\n",
    "\n",
    "        n = random.uniform(0, sum(policy[s]))\n",
    "        top_range = 0\n",
    "        for a, prob in enumerate(policy[s]):\n",
    "            top_range += prob\n",
    "            if n < top_range:\n",
    "                action = a\n",
    "                break\n",
    "\n",
    "        next_state, reward, finished, info = env.step(action)\n",
    "\n",
    "        frames.append({\n",
    "            'frame': env.render(mode='ansi'),\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': reward,\n",
    "            'info': info\n",
    "            }\n",
    "        )\n",
    "        state = next_state\n",
    "\n",
    "    if display:\n",
    "        clear_output(True)\n",
    "        env.render()\n",
    "        sleep(0.75)\n",
    "    return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_e_soft(environment, max_iterations=100, policy=None, epsilon=0.01, display=False):\n",
    "    if not policy:\n",
    "        policy = np.ones([environment.nS, environment.nA]) / environment.nA  # Create an empty dictionary to store state action values    \n",
    "    Q = create_state_action_dictionary(env) # Empty dictionary for storing rewards for each state-action pair\n",
    "    returns = {} # 3.\n",
    "    \n",
    "    for _ in tqdm.tqdm(range(max_iterations), 'episode (monte-carlo)'): # Looping through episodes\n",
    "        G = 0 # Store cumulative reward in G (initialized at 0)\n",
    "        episode = run_game(env=environment, policy=policy, display=display) # Store state, action and value respectively \n",
    "        \n",
    "        # for loop through reversed indices of episode array. \n",
    "        # The logic behind it being reversed is that the eventual reward would be at the end. \n",
    "        # So we have to go back from the last timestep to the first one propagating result from the future.\n",
    "        \n",
    "        for i in reversed(range(0, len(episode))):\n",
    "            s_t, a_t, r_t = episode[i]['state'], episode[i]['action'], episode[i]['reward']\n",
    "            state_action = (s_t, a_t)\n",
    "            G += r_t # Increment total reward by reward on current timestep\n",
    "            \n",
    "            if not state_action in [(x['state'], x['action']) for x in episode[0:i]]: # \n",
    "                if returns.get(state_action):\n",
    "                    returns[state_action].append(G)\n",
    "                else:\n",
    "                    returns[state_action] = [G]   \n",
    "                    \n",
    "                Q[s_t][a_t] = sum(returns[state_action]) / len(returns[state_action]) # Average reward across episodes\n",
    "                \n",
    "                Q_list = list(map(lambda x: x[1], Q[s_t].items())) # Finding the list of action-values (Q) \n",
    "                indices = [i for i, x in enumerate(Q_list) if x == max(Q_list)] # Indexes of max actions with maximum value\n",
    "                max_Q = random.choice(indices) # Choose one action (if several)\n",
    "                \n",
    "                A_star = max_Q\n",
    "                \n",
    "                for a, _ in enumerate(policy[s_t]): # Update action probability for s_t in policy\n",
    "                    if a == A_star:\n",
    "                        policy[s_t][a] = 1 - epsilon + (epsilon / abs(sum(policy[s_t])))\n",
    "                    else:\n",
    "                        policy[s_t][a] = (epsilon / abs(sum(policy[s_t])))\n",
    "\n",
    "    return policy, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Тестирование разных методов поиска оптимальной стратегии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_episodes(environment, n_episodes, policy):\n",
    "    wins = 0\n",
    "    total_reward = 0\n",
    "    for _ in tqdm.tqdm(range(n_episodes), 'episode'):\n",
    "        terminated = False\n",
    "        state = environment.reset()\n",
    "        while not terminated:\n",
    "            # Select best action to perform in a current state\n",
    "            action = np.argmax(policy[state])\n",
    "            # Perform an action an observe how environment acted in response\n",
    "            next_state, reward, terminated, info = environment.step(action)\n",
    "            # Summarize total reward\n",
    "            total_reward += reward\n",
    "            # Update current state\n",
    "            state = next_state\n",
    "            # Calculate number of wins over episodes\n",
    "            if terminated and reward == 20.0:\n",
    "                wins += 1\n",
    "    average_reward = total_reward / n_episodes\n",
    "    return wins, total_reward, average_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получим оптимальные политики тремя методами:\n",
    "- Monte Carlo\n",
    "- Policy Iteratopn\n",
    "- Value Iteration\n",
    "\n",
    "Для каждого метода проведём 100 000 эпизодов для вычисления оптимальной стратегии.\n",
    "\n",
    "Для оптимальной стратегии для каждого метода посчитаем количество побед, накопленную награду и среднюю награду за эпизод на 10 000 эпизодах."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode (monte-carlo): 100%|██████████| 100000/100000 [24:27<00:00, 68.15it/s]\n",
      "episode: 100%|██████████| 10000/10000 [00:18<00:00, 553.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Monte Carlo :: number of wins over 10000 episodes = 6002\n",
      "Monte Carlo :: average reward over 10000 episodes = -81.7488\n",
      "Monte Carlo :: total reward over 10000 episodes = -817488\n",
      "time = 1467.47 sec \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "policy iteration:   0%|          | 13/100000 [00:16<35:39:42,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluated 15 policies.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "episode: 100%|██████████| 10000/10000 [00:02<00:00, 3430.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration :: number of wins over 10000 episodes = 10000\n",
      "Policy Iteration :: average reward over 10000 episodes = 7.6832\n",
      "Policy Iteration :: total reward over 10000 episodes = 76832\n",
      "time = 16.70 sec \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "value iteration: 100%|██████████| 100000/100000 [36:26<00:00, 45.74it/s] \n",
      "episode: 100%|██████████| 10000/10000 [00:03<00:00, 3259.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Iteration :: number of wins over 10000 episodes = 10000\n",
      "Value Iteration :: average reward over 10000 episodes = 7.9348\n",
      "Value Iteration :: total reward over 10000 episodes = 79348\n",
      "time = 2186.13 sec \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Number of episodes to play\n",
    "n_episodes = 10_000\n",
    "\n",
    "# Functions to find best policy\n",
    "solvers = [\n",
    "    ('Monte Carlo', monte_carlo_e_soft),\n",
    "    ('Policy Iteration', policy_iteration),\n",
    "    ('Value Iteration', value_iteration),\n",
    "]\n",
    "\n",
    "for iteration_name, iteration_func in solvers:\n",
    "    environment = gym.make('Taxi-v3')\n",
    "    # Search for an optimal policy using policy iteration\n",
    "    start = time.time()\n",
    "    policy, V = iteration_func(environment, max_iterations=100_000)\n",
    "    done = time.time()\n",
    "    elapsed = done - start\n",
    "    # Apply best policy to the real environment\n",
    "    wins, total_reward, average_reward = play_episodes(environment, n_episodes, policy)\n",
    "    print(f'{iteration_name} :: number of wins over {n_episodes} episodes = {wins}')\n",
    "    print(f'{iteration_name} :: average reward over {n_episodes} episodes = {average_reward}')\n",
    "    print(f'{iteration_name} :: total reward over {n_episodes} episodes = {total_reward}')\n",
    "    print(f'time = {elapsed:.2f} sec \\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из трёх протестированных методов наиболее эффективным оказался метод Value Iteration. Получим новую политику и визуализируем её работу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "value iteration: 100%|██████████| 100000/100000 [37:26<00:00, 44.52it/s] \n",
      "episode: 100%|██████████| 10000/10000 [00:03<00:00, 2981.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Iteration :: number of wins over 10000 episodes = 10000\n",
      "Value Iteration :: average reward over 10000 episodes = 7.9205\n",
      "Value Iteration :: total reward over 10000 episodes = 79205\n",
      "time = 2246.16 sec \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "n_episodes = 10_000\n",
    "environment = gym.make('Taxi-v3')\n",
    "start = time.time()\n",
    "policy, V = value_iteration(environment, max_iterations=100_000)\n",
    "done = time.time()\n",
    "elapsed = done - start\n",
    "\n",
    "wins, total_reward, average_reward = play_episodes(environment, n_episodes, policy)\n",
    "print(f'{iteration_name} :: number of wins over {n_episodes} episodes = {wins}')\n",
    "print(f'{iteration_name} :: average reward over {n_episodes} episodes = {average_reward}')\n",
    "print(f'{iteration_name} :: total reward over {n_episodes} episodes = {total_reward}')\n",
    "print(f'time = {elapsed:.2f} sec \\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frames(environment, n_episodes, policy):\n",
    "    frames = [] # for animation\n",
    "    for _ in tqdm.tqdm(range(n_episodes), 'episode'):\n",
    "        terminated = False\n",
    "        state = environment.reset()\n",
    "        while not terminated:\n",
    "            action = np.argmax(policy[state])\n",
    "            next_state, reward, terminated, _ = environment.step(action)\n",
    "            frames.append({\n",
    "                'frame': environment.render(mode='ansi'),\n",
    "                'state': state,\n",
    "                'action': action,\n",
    "                'reward': reward\n",
    "                }\n",
    "            )\n",
    "            state = next_state\n",
    "    return frames\n",
    "\n",
    "\n",
    "def print_frames(frames):\n",
    "    for i, frame in enumerate(frames):\n",
    "        clear_output(wait=True)\n",
    "        print(frame['frame'])\n",
    "        print(f\"Timestep: {i + 1}\")\n",
    "        print(f\"State: {frame['state']}\")\n",
    "        print(f\"Action: {frame['action']}\")\n",
    "        print(f\"Reward: {frame['reward']}\")\n",
    "        sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[35m\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "Timestep: 15\n",
      "State: 418\n",
      "Action: 5\n",
      "Reward: 20\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('Taxi-v3')\n",
    "frames = get_frames(env, 1, policy)\n",
    "\n",
    "print_frames(frames)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "71b34f311a9d02406e3df95e5d0dc3cd52abb8c3af845d10e5eb91f9e8162ea1"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 ('venv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
